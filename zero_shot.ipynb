{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf64b8c-f873-4dee-bfb0-8b74f8a3cbda",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Alt text](https://miro.medium.com/v2/resize:fit:1400/1*z-7KBjUIuDgLYqEav-f3Tw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec19f6c-14d0-43d6-b005-cb510d803957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe2c23-12cd-402e-a149-d544076275d0",
   "metadata": {},
   "source": [
    "# Majority Vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b0e55-8db4-4bc8-a91a-7d894f11c726",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d291456-fb12-4274-ac5b-78f78de271f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b2fe0-5f50-4f4d-af95-ee3891f1a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SLICE_SIZE = 128\n",
    "OVERLAP = 0.3\n",
    "VOTE_THRESHOLD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972b32c-12a8-401c-9720-7732c84fd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to use\n",
    "MODELS = [\n",
    "    ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
    "    ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
    "    ('ViTamin-XL-384', 'datacomp1b'),\n",
    "    ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
    "    ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6ab6b-1212-4685-83ec-9c74296269b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for each class\n",
    "PROMPTS = {\n",
    "    'healthy': [\n",
    "        \"Green, normal-looking plant leaf or leaves\",\n",
    "        \"Plant foliage appearing healthy and undamaged\",\n",
    "        \"Leaf surface with natural color and texture\",\n",
    "        \"Vibrant leaf or leaves without visible issues\",\n",
    "    ],\n",
    "    'unhealthy': [\n",
    "        \"Plant leaf or leaves showing discoloration or spots\",\n",
    "        \"Foliage with visible signs of disease or stress\",\n",
    "        \"Leaf area with unusual texture or damage\",\n",
    "        \"Plant leaves displaying abnormal colors or patterns\",\n",
    "    ],\n",
    "    'background': [\n",
    "        \"Non-leaf elements such as soil, sky, or structures\",\n",
    "        \"Area without any visible plant leaves\",\n",
    "        \"Greenhouse materials or equipment\",\n",
    "        \"Plant parts other than leaves, like fruits or stems\",\n",
    "        \"Blurred or indistinct background elements\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec651049-01a7-4294-827d-ecbe9b5e191c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, pretrained):\n",
    "    \"\"\"Load a model and its associated components.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device)\n",
    "    model.eval()\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    return model, preprocess, tokenizer, device\n",
    "\n",
    "def get_slices(image, slice_size, overlap):\n",
    "    \"\"\"Slice the image into overlapping patches.\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    stride = int(slice_size * (1 - overlap))\n",
    "    slices = []\n",
    "    positions = []\n",
    "    \n",
    "    for y in range(0, height - slice_size + 1, stride):\n",
    "        for x in range(0, width - slice_size + 1, stride):\n",
    "            slice = image[y:y+slice_size, x:x+slice_size]\n",
    "            slices.append(slice)\n",
    "            positions.append((x, y))\n",
    "    \n",
    "    return slices, positions\n",
    "\n",
    "def classify_slices(model, preprocess, tokenizer, device, slices, prompts, model_name):\n",
    "    \"\"\"Classify each slice using the given model and prompts.\"\"\"\n",
    "    # Flatten the prompts dictionary into a list\n",
    "    all_prompts = [prompt for class_prompts in prompts.values() for prompt in class_prompts]\n",
    "    \n",
    "    # Encode all prompts\n",
    "    encoded_text = tokenizer(all_prompts).to(device)\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(encoded_text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    results = []\n",
    "    for slice in tqdm(slices, desc=f\"Running {model_name}\"):\n",
    "        # Preprocess and encode the image slice\n",
    "        image = preprocess(Image.fromarray(slice)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        # Calculate class scores\n",
    "        class_scores = {}\n",
    "        start_idx = 0\n",
    "        for class_name, class_prompts in prompts.items():\n",
    "            end_idx = start_idx + len(class_prompts)\n",
    "            class_scores[class_name] = similarity[0, start_idx:end_idx].sum().item()\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        # Determine the predicted class\n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        results.append(predicted_class)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "def draw_results(image, results, positions, slice_size):\n",
    "    \"\"\"Visualize the classification results on the image.\"\"\"\n",
    "    output = image.copy()\n",
    "    colors = {\n",
    "        'healthy': (0, 255, 0),  # Green\n",
    "        'unhealthy': (255, 0, 0),  # Red\n",
    "        'background': (0, 0, 255),  # Blue\n",
    "        'undecided': (255, 255, 0)  # Yellow\n",
    "    }\n",
    "    \n",
    "    for (x, y), result in zip(positions, results):\n",
    "        color = colors.get(result, (128, 128, 128))  # Default to gray if class not found\n",
    "        \n",
    "        # Draw rectangle around the slice\n",
    "        cv2.rectangle(output, (x, y), (x + slice_size, y + slice_size), (128, 128, 128), 2)\n",
    "        \n",
    "        # Draw the class label\n",
    "        if result == 'undecided':\n",
    "            label = 'X'\n",
    "        else:\n",
    "            label = result[0].upper()\n",
    "        \n",
    "        # Calculate text size and position\n",
    "        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n",
    "        text_x = x + (slice_size - text_size[0]) // 2\n",
    "        text_y = y + (slice_size + text_size[1]) // 2\n",
    "        \n",
    "        cv2.putText(output, label, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def majority_vote(all_results):\n",
    "    \"\"\"Perform majority voting across all model results.\"\"\"\n",
    "    votes = np.array(all_results)\n",
    "    majority = []\n",
    "    \n",
    "    for slice_votes in votes.T:\n",
    "        unique, counts = np.unique(slice_votes, return_counts=True)\n",
    "        winner = unique[counts.argmax()]\n",
    "        \n",
    "        if counts.max() >= VOTE_THRESHOLD:\n",
    "            majority.append(winner)\n",
    "        else:\n",
    "            majority.append('undecided')\n",
    "    \n",
    "    return majority\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Process a single image through the entire pipeline.\"\"\"\n",
    "    # Load and convert the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Slice the image\n",
    "    slices, positions = get_slices(image, SLICE_SIZE, OVERLAP)\n",
    "    \n",
    "    all_results = []\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(30, 5))\n",
    "    \n",
    "    # Process with each model\n",
    "    for i, (model_name, pretrained) in enumerate(MODELS):\n",
    "        model, preprocess, tokenizer, device = load_model(model_name, pretrained)\n",
    "        results = classify_slices(model, preprocess, tokenizer, device, slices, PROMPTS, model_name)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Visualize results for this model\n",
    "        output = draw_results(image, results, positions, SLICE_SIZE)\n",
    "        axs[i].imshow(output)\n",
    "        axs[i].set_title(model_name, fontsize=7)\n",
    "        axs[i].axis('off')\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, preprocess, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Perform majority voting\n",
    "    majority_results = majority_vote(all_results)\n",
    "    \n",
    "    # Visualize majority vote results\n",
    "    majority_output = draw_results(image, majority_results, positions, SLICE_SIZE)\n",
    "    axs[5].imshow(majority_output)\n",
    "    axs[5].set_title('Majority Vote', fontsize=10)\n",
    "    axs[5].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3040f87-5965-4547-bda2-05c1b89a58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to process an image\n",
    "process_image(\"hort-americas-growing-hydroponic-strawberries.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809b246-a37d-46cc-bc30-e146f7625cde",
   "metadata": {},
   "source": [
    "### Process a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f241141-1fdb-4f89-b8a1-f446a3f1b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff2741-cfc6-4401-b585-46239a69ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SLICE_SIZE_FHD = 384\n",
    "SLICE_SIZE_LARGE = 256\n",
    "OVERLAP = 0.2\n",
    "VOTE_THRESHOLD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9744314-0c61-4e8e-b225-1784ebd1a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to use\n",
    "MODELS = [\n",
    "    ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
    "    ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
    "    ('ViTamin-XL-384', 'datacomp1b'),\n",
    "    ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
    "    ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e5e23-ddb5-4504-9fb7-fc805efca75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for each class\n",
    "PROMPTS = {\n",
    "    'healthy': [\n",
    "        \"Green, normal-looking plant leaf or leaves\",\n",
    "        \"Plant foliage appearing healthy and undamaged\",\n",
    "        \"Leaf surface with natural color and texture\",\n",
    "        \"Vibrant leaf or leaves without visible issues\",\n",
    "    ],\n",
    "    'unhealthy': [\n",
    "        \"Plant leaf or leaves showing discoloration or spots\",\n",
    "        \"Foliage with visible signs of disease or stress\",\n",
    "        \"Leaf area with unusual texture or damage\",\n",
    "        \"Plant leaves displaying abnormal colors or patterns\",\n",
    "    ],\n",
    "    'background': [\n",
    "        \"Non-leaf elements such as soil, sky, or structures\",\n",
    "        \"Area without any visible plant leaves\",\n",
    "        \"Greenhouse materials or equipment\",\n",
    "        \"Plant parts other than leaves, like fruits or stems\",\n",
    "        \"Blurred or indistinct background elements\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1d361-bc4b-4f9e-bd56-c06ef5a0361f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, pretrained):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device)\n",
    "    model.eval()\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    return model, preprocess, tokenizer, device\n",
    "\n",
    "def get_slices(image, overlap):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Determine slice size based on image resolution\n",
    "    if height <= 1080 or width <= 1920:  # FHD or lower\n",
    "        slice_size = SLICE_SIZE_FHD\n",
    "    else:  # Larger than FHD\n",
    "        slice_size = SLICE_SIZE_LARGE\n",
    "    \n",
    "    stride = int(slice_size * (1 - overlap))\n",
    "    slices = []\n",
    "    positions = []\n",
    "    \n",
    "    for y in range(0, height - slice_size + 1, stride):\n",
    "        for x in range(0, width - slice_size + 1, stride):\n",
    "            slice = image[y:y+slice_size, x:x+slice_size]\n",
    "            slices.append(slice)\n",
    "            positions.append((x, y))\n",
    "    \n",
    "    return slices, positions, slice_size\n",
    "\n",
    "def classify_slices(model, preprocess, tokenizer, device, slices, prompts):\n",
    "    all_prompts = [prompt for class_prompts in prompts.values() for prompt in class_prompts]\n",
    "    encoded_text = tokenizer(all_prompts).to(device)\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(encoded_text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    results = []\n",
    "    for slice in slices:\n",
    "        image = preprocess(Image.fromarray(slice)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        class_scores = {}\n",
    "        start_idx = 0\n",
    "        for class_name, class_prompts in prompts.items():\n",
    "            end_idx = start_idx + len(class_prompts)\n",
    "            class_scores[class_name] = similarity[0, start_idx:end_idx].sum().item()\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        results.append(predicted_class)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def draw_results(image, results, positions, slice_size):\n",
    "    output = image.copy()\n",
    "    colors = {\n",
    "        'healthy': (0, 255, 0),\n",
    "        'unhealthy': (255, 0, 0),\n",
    "        'background': (0, 0, 255),\n",
    "        'undecided': (255, 255, 0)\n",
    "    }\n",
    "    \n",
    "    for (x, y), result in zip(positions, results):\n",
    "        color = colors.get(result, (128, 128, 128))\n",
    "        cv2.rectangle(output, (x, y), (x + slice_size, y + slice_size), (128, 128, 128), 2)\n",
    "        label = 'X' if result == 'undecided' else result[0].upper()\n",
    "        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n",
    "        text_x = x + (slice_size - text_size[0]) // 2\n",
    "        text_y = y + (slice_size + text_size[1]) // 2\n",
    "        cv2.putText(output, label, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def majority_vote(all_results):\n",
    "    votes = np.array(all_results)\n",
    "    majority = []\n",
    "    \n",
    "    for slice_votes in votes.T:\n",
    "        unique, counts = np.unique(slice_votes, return_counts=True)\n",
    "        winner = unique[counts.argmax()]\n",
    "        \n",
    "        if counts.max() >= VOTE_THRESHOLD:\n",
    "            majority.append(winner)\n",
    "        else:\n",
    "            majority.append('undecided')\n",
    "    \n",
    "    return majority\n",
    "\n",
    "def process_image(image_path, output_dir):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    slices, positions, slice_size = get_slices(image, OVERLAP)\n",
    "    \n",
    "    all_results = []\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(30, 5))\n",
    "    \n",
    "    for i, (model_name, pretrained) in enumerate(MODELS):\n",
    "        model, preprocess, tokenizer, device = load_model(model_name, pretrained)\n",
    "        results = classify_slices(model, preprocess, tokenizer, device, slices, PROMPTS)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        output = draw_results(image, results, positions, slice_size)\n",
    "        axs[i].imshow(output)\n",
    "        axs[i].set_title(model_name, fontsize=7)\n",
    "        axs[i].axis('off')\n",
    "        \n",
    "        del model, preprocess, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    majority_results = majority_vote(all_results)\n",
    "    \n",
    "    majority_output = draw_results(image, majority_results, positions, slice_size)\n",
    "    axs[5].imshow(majority_output)\n",
    "    axs[5].set_title('Majority Vote', fontsize=10)\n",
    "    axs[5].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_filename = os.path.join(output_dir, os.path.basename(image_path).rsplit('.', 1)[0] + '_output.png')\n",
    "    plt.savefig(output_filename, dpi=500, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    image_files = [f for f in os.listdir(input_dir) if os.path.splitext(f.lower())[1] in image_extensions]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        process_image(image_path, output_dir)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cef1e3-187d-4765-a6c8-a5e7c74aba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input and output directories\n",
    "input_directory = \"/home/jupyter/thesis_samples\"\n",
    "output_directory = \"/home/jupyter/thesis_output\"\n",
    "\n",
    "# Process all images in the input directory and save results in the output directory\n",
    "process_directory(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
